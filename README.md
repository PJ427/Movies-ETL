# Movies-ETL

## Overview 

Using the Extract, Transform, Load (ETL) process to create data pipelines to move data from a source to a destination, transforming and cleaning the data along the way.

We are to gather data from both Wikipedia and Kaggle, combine them, and save them into a SQL database so that the hackathon participants have a nice, clean dataset to use. To do this, we will follow the ETL process: extract the Wikipedia and Kaggle data from their respective files, transform the datasets by cleaning them up and joining them together, and load the cleaned dataset into a SQL database.

## Summary

After providing an intial dataset we created an automated pipeline that takes new data, perfomrs the appropriate tranformations, and load the data into existing tables. 
